# 1.26 PP-FCIL论文精读

### 联邦类增量学习：动态的联邦学习？

**摘要**：联邦学习（FL）提供了一个协作培训框架，聚合来自分散客户端的模型参数。然而，许多现有模型假设 FLa 内静态的、预定的数据类，这通常是不切实际的假设。**由于灾难性的遗忘，来自客户的实时数据添加可能会降低已建立类别的全局模型识别能力。当以前的参与者不熟悉的新客户零星加入时，这种情况会更加严重。**此外，客户数据隐私也势在必行。为了解决这些问题，我们提出了隐私保护联邦类增量学习（PP-FCIL）方法。这种方法确保了内容级别的隐私，并显着降低了 FCIL 中灾难性遗忘的风险。据我们所知，这是第一项旨在将差异隐私嵌入到 FCIL 设置中的研究。**具体来说，我们引入了一种双模型结构，利用新旧知识的自适应融合来获得新的全局模型。我们还提出了一种多因素动态加权聚合策略，考虑模型的数据不平衡时效性等多个因素，以加快全局模型聚合和准确性。在隐私保护方面，我们利用贝叶斯差分隐私为不同数据集提供更加均衡的隐私保护。**最后，我们在 CIFAR-100 和 ImageNet 上进行了实验，将我们的方法与其他方法进行比较并验证其优越性。

- ##### Q：这篇论文讲了什么？

  - 这篇论文主要研究在联邦学习（FL）中数据类别动态变化和客户端数据隐私保护的挑战
  - 论文提出了一种新的方法：隐私保护的联合增量学习（PP-FCIL）。这种方法结合了双模型结构、贝叶斯差分隐私和多因素动态加权聚合策略
    - 双模型结构有助于平衡新旧知识，减少灾难性遗忘
    - 贝叶斯差分隐私为不同数据集提供平衡的隐私保护
    - 多因素动态加权聚合策略考虑了数据不平衡和模型时效性因素，以增强全局模型聚合和准确性
- ##### Q：什么是增量学习？

  - 增量学习是一种机器学习方法，旨在使模型能够逐渐学习新数据或任务，同时**保留对先前学习的知识的记忆**

  - 与传统的机器学习的区别

    - 增量学习允许模型逐步学习，随着时间推移不断地接收新数据

    - 传统的机器学习通常需要一次性地使用所有可用数据来训练模型，也就是在训练过程开始时，所有的数据都必须准备好并同时使用

  - 它**解决了数据存储的限制，减少了重复训练的需要**，并助力于模型适应新信息而不遗忘旧信息

  - 这种学习方式对于实时更新的应用场景，如在线推荐系统或监控系统等，尤为重要
- ##### Q：什么是联邦类增量学习？

  - 联邦类增量学习（FCIL）的设置中，每个本地客户可以根据自己的偏好不断收集训练数据，而未见过新课程的新客户可以随时加入FL培训
- ##### Q：什么是灾难性遗忘？

  - 灾难性遗忘（CF）是指在原始任务上训练的神经网络在新任务上训练后崩溃并降低其在原始任务上的性能
  - CF的作用是限制神经网络持续学习的能力以及在不断变化的环境中适应新任务同时保留旧任务知识的能力
- ##### Q：何时会发生灾难性遗忘？

  - 当神经网络的权重受到新任务的数据和目标的影响，导致原始任务的知识被破坏或覆盖时，就会发生CF
  - 在实际应用中，一些客户加入FL培训的时机是灵活的，他们可能会引入其他客户看不到的新类别数据。在这种情况下使用现有的FL方法也可能导致CF
- ##### Q：什么是双模型？

  - 双模型结构通常用于处理机器学习中的某些挑战，如在增量学习环境中的知识保存
  - 这种结构中用两个模型来实现特定目标
    - 一个模型可能专注于学习新的数据或类别，而另一个模型则专注于保持对旧数据或类别的知识
  - 在隐私保护的联合增量学习中，双模型结构可能用于同时保护数据隐私和提高学习效率
- ##### Q：双模型的优缺点？

  - 优点：
    - 灵活性和效率：可以针对不同的任务定制不同的模型，增强整体系统的灵活性和效率
    - 减少遗忘：在增量学习中，一个模型可以专注于新数据，另一个保留对旧数据的记忆，减少所谓的“灾难性遗忘”
    - 专业化：每个模型可以专注于特定类型的数据或任务，提高特定领域的性能

  - 缺点：
    - 资源消耗：运行和维护两个模型可能比一个模型更耗费计算资源
    - 复杂性增加：双模型结构增加了系统的复杂性，可能需要更精细的调整和优化
    - 协调难度：确保两个模型有效协同工作可能是一个挑战，特别是在它们需要交换信息或共同作出决策时
- ##### Q：作者怎么保持双模型结构的稳定性？

  - 使用模型压缩来保持双模型结构的稳定性并减少客户端的内存压力
- ##### Q：作者怎么使用双模型结构的？

  - 双模型策略包括两个特征提取器，即 old 和 new ，以及双模型自适应特征融合模块 dAff 
    - 在客户端 k 的第 t 轮，冻结之前在第 t-1 轮训练的模型，以保留现有信息

  - 再引入一个可训练的网络 new ，它具有一致的架构来学习新知识


<img src="fig/截屏2024-01-28 21.06.10.png" alt="截屏2024-01-28 21.06.10" style="zoom:50%;" />

- ##### Q：贝叶斯差分隐私和拉普拉斯差分隐私的区别？（==可考虑使用贝叶斯差分隐私==）

  - 贝叶斯差分隐私：这种方法通常涉及使用贝叶斯统计技术来估计数据集的隐私损失。它可以提供一种更灵活的隐私保护机制，允许根据数据的特定特性或上下文调整隐私保护级别。这种方法可能更适用于复杂的数据分析场景

  - 拉普拉斯差分隐私：这是一种更传统的差分隐私方法，通过向数据添加拉普拉斯噪声来保护隐私。这种方法相对简单，易于实施，通常用于提供固定级别的隐私保护。它适用于需要固定隐私保障水平的标准化场景
  - 贝叶斯差分隐私在某些方面可能更灵活和适应性强，但实现起来可能更复杂；拉普拉斯差分隐私在实现上更直接、简单，但可能不如贝叶斯方法在特定情况下灵活
  - 基于DP的学习算法的现有工作包括本地DP（LDP）、基于DP的分布式SGD、DP元学习

- ##### Q：作者为何要使用贝叶斯差分隐私？
  
  - 更灵活，可根据数据分布修正噪声强度，为不同数据集提供更均衡的隐私保护
  
- ##### Q：作者是如何使用贝叶斯差分隐私的？
  
  - 贝叶斯差分隐私的关键特点是基于客户本地数据分布来添加噪声，使噪声添加更加合理
  - 论文提出了一种本地贝叶斯隐私保护方案，利用BDP的原理和高级组合定理来监控客户在FCIL中的隐私损失
    - 隐私损失是一个衡量差分隐私中个人隐私泄露风险的指标
    - 在差分隐私的上下文中，隐私损失表示当给定数据集中的单个数据项发生变化时，输出（如查询结果或模型预测）分布的变化程度
  - 具体实施步骤包括：
    1. 计算每次迭代的隐私成本：利用数据分布μ(x)和高级组合定理监测联合增量学习（FCIL）中客户的隐私损失
    2. 子采样的高斯噪声机制：用于FCIL的隐私保护，计算输出分布 p(ω(t)ω(t−1) D′)
    3. 根据贝叶斯差分隐私的原理：确定每次迭代的隐私成本，累计整个学习过程的隐私成本
       - 每当执行一次数据处理或查询操作，都会消耗一部分隐私预算，这部分消耗就是隐私成本
    4. 确保整个学习过程的隐私成本不超过预定的隐私预算：保证学习过程中隐私成本的累积不超出设定的隐私预算
       - ε>0为隐私预算，可以表示隐私保护的程度，其越小，隐私保护越好，但加入的噪声越多，数据可用性下降
  
  - 通过对模型参数引入噪声，攻击者无法通过查询模型获得精确的输出结果，从而防止他们恢复训练数据或根据输出结果推断特定样本是否属于模型的训练数据
  
- ##### Q：多因素动态加权聚合策略是什么？

  - 旨在提高全局模型的聚合效率和准确性
  - 在这种策略中，参与聚合的每个客户端（或节点）的贡献被根据多个因素动态加权
  - 这些因素通常包括：
    1. 数据量大小：客户端拥有的数据量可能影响其在全局模型中的权重。拥有更多数据的客户端可能会被赋予更大的权重

    2. 数据质量：数据的质量、多样性或代表性也是一个重要因素。高质量或多样化的数据可能会使客户端获得更大的权重

    3. 模型性能：客户端的本地模型性能也可能影响其在全局聚合中的权重。性能较好的模型可能会有更大的影响力

    4. 参与频率：频繁参与联合学习过程的客户端可能会获得更高的权重，以反映其对全局模型的持续贡献

    5. 时效性：客户端的数据时效性也可能是一个考虑因素。最新的数据可能会被认为更有价值，因此有更高的权重

  - 通过这种方法，联合学习系统可以更有效地处理数据不平衡、非独立同分布（non-IID）数据等问题，并提高整体学习过程的效率和准确性。这种策略特别适用于大规模、分布式的机器学习环境，其中参与节点可能具有不同的数据特性和学习能力

- ##### Q：作者是怎么使用多因素动态加权聚合策略的？

  1. 选择客户端：使用马氏距离基于数据分布选择参与聚合的客户端

  2. 权重计算：根据客户端的数据量、数据质量、模型性能等因素计算权重

  3. 动态调整：根据客户端的参与频率和数据时效性等因素动态调整权重

  4. 模型聚合：根据计算出的权重合并各个客户端的模型更新，形成全局模型

  - 这种策略通过考虑多个因素来优化全局模型的聚合过程，从而提高联合学习的效率和准确性

- ##### Q：具体代码实现？

  - 计算在给定轮次中每个选定客户端的权重比例

    ```python
    def rounds_ratio(SCt, rkt):
        """
        算法 1：Rounds_ratio
        计算每个客户端在给定回合的权重比例
    
        param SCt： 第 t 轮选定客户的集合
        param rkt： 客户 k 在第 t 轮之前参与聚合的频率
        return： 每个客户的权重比例字典
        """
        rt = 0
        Rtk = {}
    
        # 计算总参与频率
        for k in SCt:
            rt += rkt[k]
    
        # 计算每个客户的权重比例
        for k in SCt:
            Rtk[k] = (rkt[k] / rt + 1) / 2
    
        # 将比例集合起来
        Ft = {k: Rtk[k] for k in SCt}
    
        return Ft
    
    # 使用示例
    selected_clients = ['client1', 'client2', 'client3']
    client_participation = {'client1': 10, 'client2': 20, 'client3': 30}
    
    # 计算权重比例
    rounds_ratio_result = rounds_ratio(selected_clients, client_participation)
    rounds_ratio_result
    ```

  - 联邦学习中服务端学习的过程

    ```python
    def sever_execution(K, F, client_update):
        """
        算法 2：SeverExecution
        在联合学习中执行服务器端算法
    
        param K：客户端总数
        param F：参与比例
        param client_update： 表示执行客户端更新算法的函数
        return： 全局模型 ωt
        """
        a, b, c, d = 1, 1, 1, 1  # 初始化常数 a, b, c, d
        tk = {k: 0 for k in range(1, K+1)}  # 每个客户端最后一次参与的时间
        rkt = {k: 0 for k in range(1, K+1)}  # 每个客户的参与频率
    
        # 每轮的主循环
        for t in range(1, K+1):
            m = max(int(K * F), 1)  # 确定参与客户的数量
            # 应根据 SD1、SD2......、SDk 计算出的 Mt 的占位符
            Mt = []  # 需要根据具体情况进行定义
    
            # 从 Mt 中选择前 m 个客户
            SCt = Mt[:m]  # 选择逻辑的占位符
    
            # 计算轮次比率
            Ft = rounds_ratio(SCt, rkt)
    
            beta_t = 0
            omega_t = 0  # 初始化全局模型
    
            for k in SCt:
                omega_kt, gamma_kt = client_update(k)  # 执行客户端更新算法
                beta_t += beta_kt  # beta_kt 计算的占位符
                T_kt = (2.71828 ** 2) ** -(t - tk[k])  # 指数衰减
                Atk = beta_kt / beta_t  # Atk 计算的占位符
                Rtk = Ft[k]
    
                # 更新全局模型
                omega_t += (a * gamma_kt + b * abs(Dtk) / abs(Dt) * T_kt + c * Atk + d * Rtk) * omega_kt  # Dtk 和 Dt 的占位符
    
            # 更新每个客户的 Rkt 和 Tk
            for k in SCt:
                rkt[k] += 1
                tk[k] = t
    
        return omega_t  # 返回全局模型
    
    # 客户端更新函数的占位符
    def client_update(k):
        # 实现客户端更新逻辑
        return omega_kt, gamma_kt  # 返回更新的模型和附加指标
    
    # 使用示例
    K = 100  # 客户总数
    F = 0.1  # 参与比例
    
    global_model = sever_execution(K, F, client_update)
    # global_model 现在保存服务器执行后更新的全局模型
    ```

  - 联邦学习中客户端的更新过程

    ```python
    def client_update(round_t, Skt_minus_1, old_module, FCt_minus_1, Dtk, theta_kt_minus_1, epochs, augmentation_data, LM, alpha, lambda_val, rho):
        """
        算法3: ClientUpdate
        在联合学习中执行客户端的更新操作。
    
        param round_t: 当前轮次。
        param Skt_minus_1: 上一轮的范例样本。
        param old_module: 上一轮的压缩模块。
        param FCt_minus_1: 上一轮的FC层。
        param Dtk: 当前客户端数据。
        param theta_kt_minus_1: 客户端的上一theta值。
        param epochs: 训练周期数。
        param augmentation_data: 数据增强函数。
        param LM: 模型压缩方法。
        param alpha: 计算数据平衡权重因子的超参数。
        param lambda_val: 损失计算的超参数。
        param rho: 损失计算的超参数。
        return: 更新的本地模型，数据平衡权重因子，新的FC层。
        """
        # 从服务器获取全局模型
        omega_t_minus_1 = get_global_model(round_t - 1)
    
        # 计算数据平衡权重因子
        if Dtk:
            Ikt = len(Dtk) / (len(Dtk) + len(theta_kt_minus_1))
            theta_kt = theta_kt_minus_1 - (Ikt * theta_kt_minus_1)
        else:
            theta_kt = round_t
    
        gamma_kt = (math.exp(theta_kt + alpha) - math.exp(-(theta_kt + alpha))) / \
                   (math.exp(theta_kt + alpha) + math.exp(-(theta_kt + alpha)))
    
        # 准备训练数据
        Vkt = Dtk.union(Skt_minus_1)
    
        # 训练循环
        for _ in range(epochs):
            for x in mini_batches(Vkt):
                x_prime = augmentation_data(x)
                Fold = old_module(x)
                Fnew = new_module(x)  # 新模块函数的占位符
                Fmix = dAff(Fold, Fnew)  # dAff函数的占位符
    
                Zmix = FCtk(Fmix)  # FCtk函数的占位符
                Zold = FCt_minus_1(Fold)
    
                # 计算损失
                L = (1 - lambda_val) * LC(Zmix) + lambda_val * LD(Zmix, Zold) + rho * LS(Gt(Fnew), Gt(new_module(x_prime)))
    
                # 更新本地模型 ωk_t
                # 本地模型更新逻辑的占位符
    
        # 压缩模型
        compressed_model = LM(omega_kt)  # 模型压缩方法的占位符
    
        # 应用本地贝叶斯差分隐私
        omega_k_star_t = apply_bayesian_dp(compressed_model)  # 贝叶斯差分隐私应用的占位符
    
        # 更新范例样本
        stk = get_samples(Dtk)  # 提取样本的占位符
        Skt = stk.union(Skt_minus_1)
    
        return omega_k_star_t, gamma_kt, FCt_minus_1
      
    # 占位符函数
    def get_global_model(round):
        # 检索指定轮次的全局模型
        pass
    
    def mini_batches(data):
        # 将数据分割为小批量进行训练
        pass
    
    def new_module(data):
        # 使用新模块处理数据
        pass
    
    def dAff(Fold, Fnew):
        # dAff函数实现
        pass
    
    def FCtk(data):
        # FCtk函数实现
        pass
    
    def LC(Zmix):
        # LC损失计算
        pass
    
    def LD(Zmix, Zold):
        # LD损失计算
        pass
    
    def LS(Gt_new, Gt_prime):
        # LS损失计算
        pass
      
    def apply_bayesian_dp(model):
        # 贝叶斯差分隐私应用
        pass
    
    def get_samples(data):
        # 从数据中提取样本
        pass
    ```

    
