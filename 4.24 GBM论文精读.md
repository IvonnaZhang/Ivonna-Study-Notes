# 4.24 GBM论文精读

**Cited by 114**

**摘要**：多任务学习（MTL）已被广泛应用于自然语言处理领域。一个主要任务及其相关的辅助任务共享同一个编码器；因此，MTL 编码器可以学习主要任务和辅助任务之间的共享抽象信息。然后在共享编码器上使用特定任务塔来学习特定任务信息。以前的研究表明，在特定任务塔之间交换信息会产生额外的收益。这就是所谓的软参数共享 MTL。在本文中，我们为 MTL 塔之间的桥接提出了一种新颖的**门控机制**。我们根据基于方面的情感分析和顺序隐喻识别任务对我们的方法进行了评估。实验证明，在这两项任务中，我们的方法都能比基线方法获得更好的性能。基于相同的 Transformer 主干网，我们将我们的门控机制与其他信息转换机制进行了比较，例如交叉缝合、注意力和 vanilla 门控。实验表明，我们的方法也超越了这些基线方法。

- ##### Q：论文做了什么？

  - 提出了门控桥接机制（Gated Bridging Mechanism, GBM），用于多任务学习（MTL）中任务特定塔（task-specific tower）之间的信息交互

    - 该方法旨在通过**选择性过滤**和**融合信息**，提升两个自然语言处理任务的性能

    - 门控机制能直观地实现从辅助任务到主任务的信息过滤——主任务塔可通过门控自主决定从私有塔和相邻塔（辅助任务塔）中调用多少信息

  - 通过实验验证了GBM在ABSA和SMI任务上的有效性，并对比了其与基线方法（如交叉缝合、注意力机制、普通门控）的优劣表明，GBM显著优于现有方法。

    - 基于方面的情感分析（ABSA）：识别文本中的方面词（aspect）、观点词（opinion）及其情感极性（sentiment）
    - 序列隐喻识别（SMI）：判断文本中每个词语是否为隐喻

- ##### Q：论文解决了什么问题？

  - 使主任务塔能**动态过滤**辅助任务的信息，仅保留有用部分，同时将不同任务的特征投影到统一空间以增强融合效果

- ##### Q：怎么解决的？

  - 通过重置门（Reset Gate）和更新门（Update Gate）实现信息的动态过滤与融合

    - 重置门负责过滤辅助任务信息，仅保留相关部分（通过Sigmoid激活控制信息流）

    - 再使用非线性投影将辅助任务的特征投影到主任务向量空间（解决空间不一致问题）

    - 更新门负责动态决定融合比例，平衡主任务私有信息与过滤后的辅助信息

    - 最终，通过加权和池化（Weighted Sum Pooling）整合不同层的特征，进一步优化任务表现

- ##### Q：论文提出的契机？

  1. 现有方法的不足

     - 交叉缝合（Cross-stitch）直接线性融合特征，假设所有信息同等重要

     - 注意力机制（Attention）需依赖辅助任务提供有效信息，无法主动过滤噪声

     - 普通门控（Vanilla Gating）未考虑向量空间差异，可能导致特征不匹配

  2. 实际需求

     - 在ABSA中，不同子任务（如方面提取、情感分类）需要互补信息

     - 在SMI中，辅助任务（如词性标注）可能提供有用线索，但需避免引入噪声（如随机生成的隐藏状态）

  - GBM的提出正是为了解决上述局限性，在融合前增设门控过滤相邻塔中的无效信息，实现更精细的信息交互
    - 其输出值由私有塔信息与过滤后的相邻塔信息动态权衡生成，这种选择性吸收机制使MTL模型能有效利用辅助任务信息，从而提升性能

- ##### Q：多任务机制的好处？

  - 相较于单任务独立处理，引入相关辅助任务能进一步提升基于深度神经网络（DNN）的MTL模型在主任务上的性能
    - 因为多任务协同促使MTL模型的共享编码器从各子任务中获取知识，主任务通过应用这些跨任务知识得到增强
  - 降低DNN过拟合风险

- ##### Q：重置门如何过滤辅助任务信息？

  1. 输入与线性变换

     接收辅助任务塔的隐藏状态$H_{i-1}^{\tau_{m}}$，通过可学习的权重矩阵$W_{\phi_{R,i,j}}^{m}$和偏置$b_{\phi_{R,i,j}}^{m}$进行线性变换：
     $$
     \text{Linear Output} = W_{\phi_{R,i,j}}^{m} H_{i-1}^{\tau_{m}} + b_{\phi_{R,i,j}}^{m}
     $$
     
  2. Sigmoid激活
  
     对线性变换结果应用 Sigmoid 函数，生成 0 到 1 之间的权重矩阵$R_{i}^{m}$：
     $$
     R_{i}^{m} = \sigma(\text{Linear Output})
     $$
  
     - 权重接近 1：保留对应位置的辅助任务信息 
     - 权重接近 0：过滤对应位置的辅助任务信息
  
  3. 逐元素过滤
  
     将权重矩阵$R_{i}^{m}$与辅助任务隐藏状态$H_{i-1}^{\tau_{m}}$逐元素相乘（Hadamard积）：
     $$
     \text{Filtered Info} = R_{i}^{m} \odot H_{i-1}^{\tau_{m}}
     $$
     这一步直接屏蔽了权重接近 0 的无效信息。
  
  4. 非线性投影
  
     将过滤后的信息通过 tanh 激活函数映射到主任务的向量空间，解决不同任务特征空间不一致问题：
     $$
     C_{i}^{m} = \tanh(W_{\phi_{C,i,j}}^{m} \cdot \text{Filtered Info} + b_{\phi_{C,i,j}}^{m})
     $$
  
- ##### Q：更新门如何动态决定融合比例？

  1. 输入与联合变换

     接收两部分输入：  

     - 主任务私有信息：主任务塔的隐藏状态$H_{i-1}^{\tau_{j}}$  
     - 过滤后的辅助信息：经过重置门处理后的$C_{i}^{m}$

     对两者进行联合线性变换（含可学习参数$W, V, d$）
     $$
     \text{Joint Input} = W_{\phi_{Z,i,j}}^{m} H_{i-1}^{\tau_{j}} + V_{\phi_{Z,i,j}}^{m} C_{i}^{m} + d_{\phi_{Z,i,j}}^{m}
     $$

  2. Sigmoid激活

     生成 0 到 1 之间的融合权重矩阵$Z_{i}^{m}$：
     $$
     Z_{i}^{m} = \sigma(\text{Joint Input})
     $$

     - 权重接近 1：保留更多主任务原有信息
     - 权重接近 0：更多吸收辅助任务的新信息

  3. 动态融合

     按权重$Z_{i}^{m}$和$1-Z_{i}^{m}$对主任务信息与辅助任务信息进行加权融合：
     $$
     F_{i}^{m} = Z_{i}^{m} \odot H_{i-1}^{\tau_{j}} + (1 - Z_{i}^{m}) \odot C_{i}^{m}
     $$

     - 示例：若$Z_{i}^{m}=0.8$，则 80% 保留主任务信息，20% 吸收辅助信息

- ##### Q：重置门与更新门的核心区别与作用是什么？

  | **组件**   | **功能**                                           | **数学操作**                                                | **输出意义**              |
  | ---------- | -------------------------------------------------- | ----------------------------------------------------------- | ------------------------- |
  | **重置门** | 过滤辅助任务信息，解决噪声问题                     | $R_{i}^{m} \odot H_{i-1}^{\tau_{m}}$                        | 0-1权重，屏蔽无效信息     |
  | **更新门** | 动态平衡主任务与辅助任务信息，解决信息融合比例问题 | $Z_{i}^{m} \odot \text{主} + (1-Z_{i}^{m}) \odot \text{辅}$ | 0-1权重，决定新旧信息比例 |

  - 直观理解

    - 重置门：像“筛子”一样过滤辅助任务信息，只让有用的部分通过

    - 更新门：像“调音旋钮”一样，动态调整主任务与辅助任务信息的混合比例，确保融合后的特征最优

- ##### Q：模型的实现流程是什么样的？

  1. 共享编码层
     - 使用BERT作为共享编码器，生成初始隐藏状态$H^s$
  
  2. 任务特定塔（Task-specific Tower）
     - 每个任务塔包含多个块（Block），每个块由GBM层和Transformer层组成
     - Block 0：仅包含Transformer层，用于初始化任务特征
     - Block i (i > 0)：
       - 输入：前一层各任务塔的隐藏状态$H_{i-1}^{\tau_j}$
       - GBM操作：
         - 通过重置门$R_i^m$过滤其他任务的信息
         - 通过非线性投影（tanh）将辅助任务特征映射到主任务空间
         - 通过更新门$Z_i^m$动态融合主任务私有信息与过滤后的辅助信息
       - 输出：融合后的特征$G_i^{\tau_j}$，输入到下一层Transformer
  
  3. 加权和池化（Weighted Sum Pooling）
     - 整合不同块的特征，动态学习各层权重$\alpha_i^{\tau_j}$，生成最终特征表示$H_{pool}^{\tau_j}$
  
  4. 任务输出
     - 线性投影层将池化后的特征映射到标签空间，通过交叉熵损失优化多任务目标

<img src="/Users/zhangyuxin/Desktop/Ivonna-Study-Notes/fig/截屏2025-04-26 15.44.57.png" style="zoom:50%;" />