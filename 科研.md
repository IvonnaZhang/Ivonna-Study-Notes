# 深度学习

## 参数Parameters

也叫权重(weights)

- 比如全连接层中的权重矩阵、卷积层中的卷积核都是需要训练的参数

- 参数需要从训练数据中学习
- 训练时，我们用梯度下降等算法更新参数
- 在训练好模型之后，要在测试集上评价模型的好坏
- 神经网络的参数依赖与训练数据和超参数



## 超参数 Hyper-parameters

- 搭建神经网络和开始训练之前就要手动设置的一些数值
- 对学到的神经网络参数有影响，进而影响在测试集上的准确率

### 神经网络结构Architecture

有多少卷积层、每层有多少卷积核、每个卷积核有多大

### 优化算法Algorithm

比如是用SGD还是什么算法

算法中的学习率、batch大小、epoch的数量



## 卷积神经网络 CNN

只要是类图像特征(两行或两列互换后会导致信息量的破坏和减少)的数据就可以处理，如视频、音频、文本等

### 卷积 Convolution

- 识别出新图的特征，比较它们与原图的相似性；提取出的新特征叫做卷积核，也叫特征提取器

- 卷积核的生成方式一般是随机的或全0，然后让算法自己去纠正卷积核究竟是什么；又或者通过预训练的方式提取卷积核

- 用某个卷积核对原图进行卷积操作，就相当于将原图中包含这个特征的特征（与卷积和一样的特征）提取了出来


### 池化 Pooling

- 池化又叫做降采样和子采样，目的是将得到的feature map进行缩小。会牺牲一部分信息，但是是在接受范围内的。一般用最大池化Max Pooling（框中最大值代表整个框）和均池化（框中所有数的均值代表整个框）
- 目的是降低计算量，提高计算速度

### Zero Padding

- 图像的边缘地区也充满特征，如果要提取图片边缘特征，就要在图片最外圈补0

### 非线性激活函数 ReLU

- ReLU=max(0,x) 大于零就是他本身，小于零就把它全部变成0

- 在梯度下降时比其他的激活函数好很多

- 一般放在卷积层后，将卷积后得到的负数全部变成0

### 全连接层 Fully connected layer

- 将得到的所有矩阵排成一列，每个值(像素点)都乘以各自的权重后再相加。得到的数值就是该图是原图的概率
- 权重可以通过训练(BP算法)得到；权重是神经网络利用特征学习到的

### 损失函数Loss Function

- 将得到的结果与真实的结果进行一个误差的计算
- 目标是通过修改卷积核的参数，修改全连接每一个神经元的权重将损失函数降到最小
- 找到损失函数的最小值意味着学习成功

### 反向传播算法

- 一层一层将误差反馈回去进行修改
- 初始化卷积核和权重参数是随机的，后续反向传播会修正他们
- 通过不停的训练，他就能自己学会采用哪些卷积核，全连接网络中的每一个神经元的权重是多少

### 超参数 Hyper-parameters(keobs)

- 搭建神经网络和开始训练之前就要手动设置的一些数值

- 人为定好卷积核的尺寸(Size of features)、卷积核的数目(Number of features)、池化的步长(Window stride)、池化的大小(Window size)、全连接层神经元的数量(Number of neurons)

- 好的初始化可以更接近正确答案



## 联邦学习

- 是一种分布式机器学习
- 目标是让多个用户一起协同训练出一个模型，但这些用户不共享数据，==**数据不能离开用户本地**==
- 目的是为了解决数据的协作和隐私问题
- 在联邦学习中，用户先在本地训练数据形成一个本地模型，然后把模型上传到云端服务器进行聚合。聚合后把聚合的模型返回给用户端。用户端可以根据返回的模型来调整自己的模型
- 2016年Google为了解决安卓系统的更新问题，提出让用户在自己的设备中训练模型，以上传模型参数取代直接上传数据，一定程度上保证了个人数据的私密。这就是联邦学习的雏形
- worker节点不稳定，sever发一个请求worker未必会立即响应，这对分布式计算造成了困难
- 通信代价非常大，远大于计算的代价
- **数据并非独立同分布**
- **通信很慢**
- 节点负载不平衡
- 研究方向：
  - Communication- efficient algorithms：降低通讯次数(多计算少通信，如FedAvg)
  - Defense against privacy leakage：保护隐私，阻止别人通过梯度或者模型参数来反向推断出用户数据
    - Differential Privacy：确实可以有效的保护隐私，但是加noise会掉几个百分点，工业界很难接受
    - ==**随机矩阵变换**==的方法来阻止反向推断用户数据，同时又不影响模型的准确度
  - Robustness to Byzantine faults：提高算法鲁棒性
    - 有多个用户参与学习，每个用户都不受控制，有可能出现**拜占庭将军问题**：中间出现叛徒，叛徒发送乱七八糟的错误信息给sever，也可以精心设计出有毒的样本或者有毒的梯度来毒害模型
    - 如果数据是独立同分布的，就有办法检测出异常的worker节点，可以发现叛徒，也有办法抵御攻击

### 联邦学习步骤



### 横向联邦学习 (特征对齐的联邦学习)

联邦学习的参与者们业务相似，数据特征重叠多，样本重叠少(如不同地区的两家银行)，可通过上传参数，在服务器中聚合更新模型，再将最新的参数下放完成模型效果的提升

### 纵向联邦学习 (样本对齐的联邦学习)

参与者的数据中样本重叠多，特征重叠少(如同一地区的银行和电商)，就需要先将样本对齐。由于不能直接比对，我们需要加密算法的帮助，让参与者在不暴露不重叠的样本的情况下，找出相同的样本后再联合它们的特征进行学习

### 联邦迁移学习

如果样本和特征重合的都不多，希望利用数据提升模型能力，就需要将参与者的模型和数据迁移到同一空间中运算

### 带个性化层的联邦学习 Federated Learning with Personalization Layers(FedPer)

#### 模型

FedPer的视图如下所示：

<img src="/Users/zhangyuxin/Library/Application Support/typora-user-images/截屏2023-08-01 17.49.23.png" alt="截屏2023-08-01 17.49.23" style="zoom: 25%;" />

所有客户端模型共享基本层（蓝色），然后由于数据分布的不同，不同客户端模型具有不同的个性化顶层。

#### 问题定义

- 参数定义
  
  $K_B$：客户端模型基本层数量
  $K_P$：客户端模型个性化层数量
  $N$：设备总数
  $a_{B,1},a_{B,2},...,a_{B,K_B}$：基本层的激活函数
  $W_B=W_{B,1},W_{B,2},...,W_{B,K_B}$：基本层权重矩阵。这里需要注意，基本层中不同层的权重矩阵可能具有不同的维度
  $W_{P_j}=W_{P_j,1},W_{P_j,2},...,W_{P_j,K_P}$：第j个客户端的个性化层的权重矩阵

​		因此，神经网络的forward pass可以描述为：
$$
\hat{y}=a_{P_j,K_P}(W_{P_j,K_P}...a_{P_j,1}(W_{P_j,1}a_{B,K_B}(W_{B,K_B}...a_{B,1}(W_{B,1}x)...))...)
$$
​		客户端的样本首先经过基本层，然后再经过个性化层，最后得到输出。forward pass可以简单描述为：
$$
\hat{y}=f(x;W_B,W_{P_j})
$$
​		不同客户端的基本层权重矩阵是一样的，个性化层权重矩阵不一样。

​		需要优化的损失函数：
$$
L^{PR}(W_B,W_{P_1},...,W_{P_N})=\frac{1}{N}\sum_
{j=1}^NE_{(x,y)\sim{P_j}}[l(y,f(x;W_B,W_{P_j}))]
$$
​		即所有客户端损失的均值。

​		其中第j个设备上的损失定义为：
$$
L_j^{ER}(W_B,W_P)\triangleq\frac{1}{n_j}\sum_{i=1}^{n_j}l(y_{j,i},f(x_{j,i};W_B,W_P)
)
$$

#### 算法

- 算法为代码

<img src="/Users/zhangyuxin/Library/Application Support/typora-user-images/截屏2023-08-02 10.02.27.png" alt="截屏2023-08-02 10.02.27" style="zoom: 50%;" />

**总体流程描述**

1. 服务器端初始化基本层权重$W_B^{(0)}$

2. 客户端初始化自己的个性化层权重$W_{P_j}^{(0)}$

3. 服务器端将$W_B^{(0)}$发送到各个客户端

4. 服务器端和客户端都执行相同的循环：客户端收到服务器传来的基本层权重，然后利用本地数据来进行SGD更新:    $SGD(L_j^{ER},\widehat{W}_B,\widehat{W}_P,\eta_j^{(k)},e,b)$

   其中$\eta_j^{(k)}$表示当前学习率，$e$为本地训练轮数，$b$为$batch~size$

1. 本地训练完毕后只将基本层权重传回服务器
2. 服务器聚合所有客户端的基本层权重，然后分发给所有客户端，进行下一轮通信

**总得来说，FedPer与FedAvg有以下3点不同**：

1. FedAvg每次需要随机选择客户端，而FedPer需要激活所有客户端
2. FedAvg需要聚合所有参数，FedPer只是聚合基础层参数
3. FedAvg中只有一个全局模型，所有客户端都使用该全局模型，而FedPer中每个客户端都有自己的个性化模型



## 线性(回归)预测 Linear Predictor

- Inputs：$x\in \R^d$
  - 线性回归的输入是一个向量 x (比如说x是一个房子的特征，这个房子有100个特征，那么x就是一个100维的向量)

- Prediction：$f(x)=x^T w=w_1x_1+w_2x_2+...+w_dx_d$
  - 其中d是特征的数量，每一个x都代表一个特征(如房间的数量、房子的年龄、占地面积等)，w都是权重(代表对应特征在房价中的权重)
  - 把x和w的内积作为预测，把这个线性函数计为$f(x)$，$f(x)$的输出记为房价
  - 向量w是函数的参数，并且w与x的维度一致



## 最小二乘回归 Least squares regression

机器学习就是要从过去的数据中学习出模型的参数w

- Training inputs/features：$x_1,...,x_n\in\R^d$

- Training targets/labels：$y_1,...,y_n\in\R$
- Loss function：$L(w)=\sum^n_{i=1}\frac{1}{2}(x_i^Tw-y_i)^2$
  - 其中$x_i$是某个房子的特征向量，$w$是模型的参数，$x_i^Tw$是模型估算出来的房价，$y_i$是房子的真实房价
  - 我们希望L越小越好，L越小证明预测的越准确
- Least squares regression：$w^*=min_wL(w)$
  - 这个问题的最优解记为$w^*$



## 梯度 Gradient

- Gradient：$g(w)=\sum_{i=1}^ng_i(w)$,  where $g_i(w)=(x_i^Tw-y_i)x_i$

  - 每个$g_i$只与这个样本的$x_i,y_i$有关，与其他的数据样本无关。这也是为什么做并行计算的时候可以将样本划分到多个机器上，每个机器只需要用本地的数据计算一个本地的$g_i$)

- Gradient descent：$w_{t+1}=w_t-\alpha·g(w_t)$

  - $\alpha$叫做步长或学习率

  - 梯度的物理意义：在一个点w上沿着一个梯度方向$g(w)$走很小一步，这个函数值$L(w)$肯定是上升的；所以朝着梯度方向$g(w)$的相反方向走一小步，函数值$L(w)$肯定是下降的。我们的目标是找到一个w使$L(w)$越小越好

- 样本越多计算量越大
- 维度越高计算量越大
- 梯度的计算量很大，计算梯度是算法的瓶颈。如果把计算梯度并行化，算法就会变得更快



## 并行计算 Parallel Computing

- cpu时间：所有参与计算的cpu的总时间
  - 并行计算可以减少钟表时间，但不会减少cpu时间

- 因为要涉及到很多个处理器，就要考虑处理器之间通信的问题
  - 通信方式
    - Share memory：一个处理器可以直接看到其他处理器得到的结果
      - 但是共享内存的通信没办法做到大规模的并行，一个主机上可以插的 处理器是有限的
    - Message passing：有多个节点，每个节点都有几个处理器。节点的处理器是可以共享内存的，但是节点1没有办法看到节点2的内存
      - 节点之间可以用网线连起来，也可以远程连接用TCPIP发消息
      - 节点之间通信要用Message passing，比如可以把向量打包成很多个package，然后通过网线或者TCPIP协议，把消息发送给另一个节点

### 加速比 Speedup Ratio

$$
speedup ratio=\frac{wall~clock~time~using~one~node}{wall~clock~time~using~m~nodes}
$$

### 	通信代价 Communication Cost

- **Communication complexity 通信复杂度**：sever和worker之间需要传输多少个word或者多少个byte
  - 通信复杂度正比于模型参数，模型越大，通信复杂度就越高
  - 节点越多，通信复杂度就越高

- **Latency 网络延迟**：通信的时候，向量和矩阵都是压缩成很多个package再通过网络传输，worker发送一个package，sever不会立马接收到，哪怕这个包只有一个byte，也需要时间传输，这个代价就叫网络延迟
  - 网络延迟是由计算机网络和软件系统决定的
  - 网络延迟跟传多少个参数没有关系，传一个矩阵和传一个byte延迟几乎是一样的
  - 每一次通信就会造成一个延迟，算法通信次数越多，延迟就越大

- **Communication time 通信时间**：$\frac{complexity}{bandwith(带宽)}+latency$

### Client-Sever架构

一的或多个节点充当sever，协调其他节点，用来存储模型参数并且更新模型参数；剩余的节点充当worker，用来做计算

sever和worker之间用Message passing来通信

主要步骤：

- worker节点向sever索要模型参数，sever会把最新的模型参数发给worker
  - 这一步需要通信，通信复杂度就是模型参数的数量
- worker就用本地的数据和最新的模型参数来算出本地的梯度或者随机梯度
  - 这一步是不需要通信的，worker只用在本地做计算就好
- worker算出梯度后把梯度发给sever
  - 梯度的维度和参数的维度是一样的，所以这一步的通信复杂度还是等于模型参数的数量
- sever收到梯度后，用梯度来更新模型参数，比如做一次梯度下降或者随机梯度下降
  - 随机梯度是用用户本地一个batch的数据算出来的
  - 梯度就是数据做了一个变换而已，梯度几乎携带了所有信息，所以拿梯度是可以反推出来数据的
    - 模型参数也带了训练数据的信息，甚至模型记住了训练数据

#### MapReduce

- 有一个节点sever，用来协调整个系统；其他的节点都作为worker，数据全都存在worker上面，计算主要是由worker来做

- **步骤**

  - Broadcast：sever和worker之间可以通信，sever可以把所有信息广播到worker节点上

  - Map：要实现算法，要自己定义一个函数，每个worker都会运行这个函数，Map是由每个worker并行做的

  - Reduce：worker把计算后的结果传回sever，然后sever将这些结果整合起来得到梯度g

  - 最后sever做一次梯度下降，对模型参数$w$进行更新，得到下一个模型参数$w_{t+1}$

  系统会不断重复这些操作直到算法收敛

- 系统需要同步

- 并行是同步的，每一轮要等到所有worker全部完成工作，才能进行下一轮

  - 会导致节点的空闲和效率低下

- 做大数据处理很好，但做机器学习效率并不高

- Message passing, Client-Sever, bulk synchronous, and data parallelism

#### Parameter Sever

- 数据并发(每个worker节点都有一部分数据)

- 有sever和worker

- 系统的效率更高，更适合机器学习
- 并行是异步的：一个worker完成计算就可以跟sever通信，不需要等其他worker完成工作
- 现在并行训练神经网络都是用这个

- 异步算法需要更多的迭代次数才能收敛，但实际上用起来会比同步算法快
- 异步算法的实现是有要求的，这些worker必须比较稳定。假如有个别worker出现慢很多很多倍的情况，收敛就会出问题
- Message passing, Client-Sever, asynchronous, and data parallelism

### Peer-to-Peer架构

- 去中心化的网络 ，点对点

- 没有sever只有worker，所有的节点都拿来做计算

- 每一个节点都有几个邻居，邻居之间可以通信且只跟连接节点通信

#### Decentralized Network

- 可以是同步的也可以是异步的，但现在理论分析基本都是同步的

- **步骤**

  - 节点用自己本地的数据和本地的参数算出本地的梯度
  - 向邻接节点要他们的模型参数
  - 将自己的参数和邻接节点的参数做一个加权平均，用这个加权平均做自己新的参数
    - 保证了最终大家的参数都会收敛到相同的地方去
  - 节点本地做一次梯度下降用来更新自己的参数

  每个节点都独立重复这四个步骤，最终算法会收敛

- 去中心化网络构成了一个图，收敛率与图的连接状况有关，图连接越紧密，算法收敛越快

  - 如果这个图是一个完全图，算法收敛会非常快
  - 如果这个图不是强连接的，也就是可以拆成两个部分，那么算法就不会收敛

- Message passing, Peer-to-Peer, synchronous or asynchronous, and data parallelism



## 分布式计算 Distributed Computing

- 分布式计算的定义比并行计算的定义更宽泛

- 可以认为分布式计算就是一种并行计算，现在学术界这两个术语都是在混用，没有一个明显的界限

- 如果节点都放在一个地方，那网线连起来，那就是并行计算，不能叫分布式计算；如果节点不在一起，就可以叫分布式计算

- 如果数据或者模型被划分到多个节点上，这几个节点用Message passing通信，就叫分布式计算；
- 假如计算都是在一个节点上完成，即使有很多个处理器，也不能叫分布式计算，只能叫并行计算



## 图对比学习 Graph Contrastive Learning

- 计算神经网络的损失函数有三个基本思路
  - 最小二乘法
  - 极大似然估计法
  - 交叉熵
    - 先将模型换成熵这么一个数值，然后利用这个数值去比较不同模型之间的差异

### 信息量 Amount of Information

- 小概率事件：带来的信息量很大

- 大概率事件：带来的信息量很小

- 信息量的大小不是看这个信息你知不知道，而是看他给你能带来多少确定性

- 独立的事件，信息量可相加

- $$
  I(x)=\log_2(\frac{1}{p(x)})=-\log_2(p(x))
  $$

### 香农熵 Shannon Entropy

- 熵在信息论中是对不确定性的测量，我们用熵来表示一个随机变量的信息量

  - 一个概率分布所包含的平均信息量就称为这个概率分布的熵

  - 概率分布的期望的平均信息量

  - $$
    H(p)=\sum p_iI_i^p=\sum p_i\log_2(\frac{1}{p_i})=-\sum p_i\log_2(p_i)
    $$

  - $$
    H(p)=\int p_iI_i^p=\int p_i\log_2(\frac{1}{p_i})=-\int p_i\log_2(p_i)
    $$

- 描述概率不确定性的方法

- 概率密度函数更加均匀（各个事件发生概率更接近），产生的随机变量的不确定性就更高，也就有更大的熵

- 概率密度函数更加聚拢（某个事件发生概率更更高），产生的随机变量的不确定性就更确定，也就有更小的熵

### 交叉熵 Cross Entropy

- 是Loss函数的一种(也称为损失函数或代价函数),用于描述模型预测值与真实值的差距大小 

- 描述了从估计概率分布的角度对真实概率分布的平均信息量的一个估计值

- q：对各事件发生的的概率的估计(估计出来的概率分布)，可看作是猜测的且待修正的

- p：是真实的概率分布，可看作通过抽样得到的

- 给定q，对真实概率分布p的平均信息量的估计叫做交叉熵

- 期望是通过真值的概率分布p来求的

  - 事件总是以真实的概率分布区发生

- 信息量的计算以估计出来的概率分布q来计算的

  - 我们只能看到估计出来的概率分布

- $$
  H(p,q)=\sum p_iI_i^q=\sum p_i\log_2(\frac{1}{q_i})=-\sum p_i\log_2(q_i)
  $$

  在神经网络中利用交叉熵：$\sum_{i=1}^m$(m是两个系统中事件数量更大的那个事件数)

  以训练模型判断照片是否为猫咪为例：<img src="/Users/zhangyuxin/Library/Application Support/typora-user-images/image-20230721163035936.png" alt="image-20230721163035936" style="zoom: 40%;" />
  ($x_i$只有两种情况，要么是猫，要么不是猫；$y_i$是在判断这个猫有多像猫)

  $P$是作为基准，是要被比较的概率模型(人脑)
  $$
  \begin{align}
  H(P,Q)
  	& = \sum_{i=1}^mp_i(-\log_2q_i)\\
  	& = \sum_{i=1}^np_i(-\log_2q_i)~把m换成n(训练时用的照片的数量)\\
  	& = -\sum_{i=1}^n(x_i·\log_2y_i+(1-x_i)·\log_2(1-y_i)~与极大似然估计法推出来的式子一样\\
  \end{align}
  $$

当$x_i=1$人脑判断是猫的时候，神经网络给出来是猫的概率$y_i$是多少;
当$(1-x_i)=1$人脑判断不是猫的时候，神经网络给出来不是猫的概率$(1-y_i)$是多少

### KL散度 Kullback–Leibler divergence

- 量化地去衡量两个概率分布的区别的函数，定量描述了两个概率分布之间的区别

- 一个概率模型中的重要基础概念，对推导模型的损失函数(如交叉熵损失函数)有着重要的意义

- KL散度=交叉熵 - 熵

- $$
  \begin{align}
  D(p||q)
  &=H(p,q)-H(p)\\
  &=\sum p_iI_i^q-\sum p_iI_i^p\\
  &=\sum p_i\log_2(\frac{1}{q_i})-\sum p_i\log_2(\frac{1}{p_i})\\
  &=-\sum p_i\log_2(\frac{p_i}{q_i})
  \end{align}
  $$

- $D(p||q)\geq0$且只有当两个概率分布完全一致的时候KL散度才会=0

- $D(p||q)\neq D(q||p)$KL散度不是距离的衡量

- 优化模型需要最小化KL散度

- $$
  \nabla_\theta D(p||q_\theta)=\nabla_\theta H(p,q_\theta)-\nabla_\theta H(p)=\nabla_\theta H(p,q_\theta)
  $$

  ​                                                                                          其中$\nabla$是求梯度，$\theta$是参数

  ​                                                                  又因为$p$是真实的概率分布，与$\theta$无关，所以$\nabla_\theta H(p)=0$

- 为了衡量两个概率分布的区别，一个直观的想法是基于概率分布p来采样一个随机数序列，然后分别求这个序列在给定概率分布p下的一个概率，和给定概率分布q下发生的一个概率。再将这两个值做一个比较，如果比较相近，则说明这两个概率分布是比较相近的；反之则相差较远
  $$
  D(p||q)=\sum p_i\log_2(\frac{p_i}{q_i})=\log(\frac{P(sequence~~of~~distribution~~p~|~distribution~~p)}{P(sequence~~of~~distribution~~p~|~distribution~~q)})
  $$

### 互信息 Mutual Information

- 两个随机变量的互信息是指变量间相互依赖的量度
- 互信息度量的是两个随机变量共享的信息
  - 知道随机变量X，对随机变量Y的不确定性减少的程度(或者知道Y后X的不确定性的减少)，用I(X;Y)表示
  - 例子：若X表示骰子掷出的点数，Y 表示X的奇偶性(当X为偶数时Y=0，当X为奇数时Y=1)。
    - 如果知道X：假设X=1，则可与判断Y=1(失去了Y=0这一信息的可能性)
    - 如果知道Y：假设Y=0，则可以判断X为偶数(失去了X=1, 3, 5 的可能性)

​			   所以可以说随机变量X、Y间存在互信息

![image-20230721151720475](/Users/zhangyuxin/Library/Application Support/typora-user-images/image-20230721151720475.png)

- $$
  I(X;Y)=H(X)-H(X|Y)=H(Y)-H(Y|X)=H(X)+H(Y)-H(X,Y)
  $$

​																 互信息=随机变量X的不确定性 - 在知道随机变量Y之后X的不确定性

- $$
  I(X;Y)=\sum_{x,y}p(x,y)\log_2(\frac{p(x,y)}{p(x)p(y)})
  $$

- 如果X、Y互为确定性函数(知道X决定Y的值，反之亦然)，那么二者的互信息与它们的熵相同:
  $$
  I(X,Y)=1=H(X)=H(Y)
  $$

- 两个相互独立的变量之间的互信息为0

- log的底数为2是因为我们用比特作为单位度量信息，每个比特可以表示两个数字



## 链路预测 Link Prediction

- 链路预测是一个利用网络做预测的经典任务
  - 所谓链路(Link)指节点与节点之间的连接，也就是图论中的边
  - 所谓链路预测就是预测原本不相连的两个节点之间是否有边存在
    - 若是在有权图中，那就顺便还要预测一下相邻边的权重
    - 如在一个社交网络图中，链路预测的任务就好比是在预测某个某个用户是否对另一个用户感兴趣，也就是好友推荐任务
- 链路预测一直不温不火的原因是跳开他一样能做推荐
  - 例如基于近邻的链路预测，其实就等同于基于近邻的协同过滤。而学习协同过滤不需要懂图论
  - 而如今图神经网络的兴起有直接导致链路预测中一些复杂的算法过时。因为图神经网络可以有效的解决90%以上的链路预测任务

- 链路预测分为两类
  - 利用已知信息预测不存在的信息
  - 利用已知信息预测未来的信息

### 实验方法

1. 明确你的数据集(Crawdad.org)
2. 明确什么是数据集，什么是测试集(一般比例是8:2 7:3 6:4)
3. 进行对比实验，一定要控制单一变量，同类方法进行比较

### 国内外研究方法

#### 指标类

- 经典指标
  - CN共同邻居
  - RA资源分配
  - AA
  - Jaccard指标
- 改进指标
  - 基于经典指标结合你所研究的对象特点、属性去改进经典指标
    - 如社区网络、科学家合作网络这些研究对象是人的网络中，有一个最明显的特点：这些节点都带有比较明显的聚簇(社团)属性

#### 机器学习

构建你所研究对象的特征属性，并输入到模型中，通过多次迭代训练，去不断地调整，或者让这个模型去自适应地调整到它的最佳参数。将调整后的参数用于预测未来未知的指标

#### 深度学习模型(神经网络模型)

由输入层、输出层、和若干个隐藏层组合的一个模型。其最关键在于隐藏层的一个设计，也就是说隐藏层才是提取你对象特征的一个关键性因素

#### 表征学习

利用表征学习模型去提取对象的特点，从而去预测节点的连接状态

比较有名的是==**word2vec**==：

- 提取词向量特征；把那些对应着词的特征的单词或语句组合在一起
- 类似一种聚类

#### 强化学习

不会直接告诉模型这两个节点之间是断开的还是连接的，而是一种通过奖励和惩罚机制来不断地调整模型自有的参数，从而以最大概率去预测节点对的连接状态

### 链路预测方法

#### 启发式方法 Heuristic Methods

启发式方法计算一些启发式图结构特征作为链接的可能性，例如常见邻居(CN)、Adamic-Adar、优先附着(PA)和Katz指数

| Name                    | Formula                                                      | Order  |
| ----------------------- | :----------------------------------------------------------- | ------ |
| Common Neighbors        | $|\Gamma(x)\cap \Gamma(y)|$                                  | first  |
| Jaccard                 | $\frac{\Gamma(x)\cap \Gamma(y)}{\Gamma(x)\cup \Gamma(y)}$    | first  |
| Preferential Attachment | $\Gamma(x)·\Gamma(y)$                                        | first  |
| Adamic-Adar             | $\sum_{z\in\Gamma(x)\cap\Gamma(y)}\frac{1}{log|\Gamma(z)|}$  | second |
| Resource  Allocation    | $\sum_{z\in\Gamma(x)\cap\Gamma(y)}\frac{1}{|\Gamma(z)|}$     | second |
| Katz                    | $\sum_{l=1}^\infty\beta^l|path(x,y)=l|=\sum_{l=1}^\infty\beta^lA^{(l)}$ | high   |
| PageRank                | $q_{xy}+q_{yx}$                                              | high   |
| SimRank                 | $\gamma\frac{\sum_{a\in\Gamma(x)}\sum_{b\in\Gamma(y)}score(a,b)}{|\Gamma(x)·\Gamma(y)|}$ | high   |
| Resistance Distance     | $\frac{1}{l_{xx}^+l_{yy}^+-2l_{xy}^+}$                       | high   |

- $\Gamma(x)$表示顶点$x$的邻居集
- $\beta<1$是一个阻尼因子
- $|path(x,y)=l|$计数$x$和$y$之间长度为$l$的路的数目
- $q_{xy}$是在从$x$开始的随机游走下$y$的平稳分布概率
- SimRank分数是一个递归定义
- $l_{xy}^+$是图的Laplacian矩阵的伪逆的$(x , y)$项

#### 嵌入方法

嵌入方法包括矩阵分解(MF)和Node2vec，以传导方式学习自由参数节点嵌入，因此不适用于未见节点和网络

#### 基于特征的方法

基于特征的方法利用显式节点特征来预测链接

- 通过以统一的方式从图结构和节点特征中学习，GNN在最近几年中在链路预测性能方面表现出了很好的性能
  - 基于GNN的链路预测方法的两个主要类别是图自编码器(GAE)和SEAL

### 路径

- 路径是从某一个节点到另一个节点之间经过的边与节点组成的子图，包括头尾节点
  - 广义的路径：环也是路径
  - 狭义的路径：路径上的节点是不能有重复的
- 一条路径上的的边数被称为路径的阶数
  - 两节点之间隔了n条边，就可以称他们互为对方的n阶邻居

### 基于相似性的链路预测

#### 基于局部信息的相似性指标

- 近邻相似性指标是衡量两个样本之间相似度的数学量，取值越大代表这两个样本越相似

- 下表列出了10种基于节点局部信息的相似性指标，第1种是计算共同邻居相似度的基本公式，其中第2至7种相似性指标是直接基于共同邻居指标的不同的规范化而得到的

  |          名称          |                             定义                             |          名称          |                             定义                             |
  | :--------------------: | :----------------------------------------------------------: | :--------------------: | :----------------------------------------------------------: |
  | ①Common Neighbors(CN)  |              $S_{xy}=|\Gamma(x)\cap \Gamma(y)|$              | ⑥大度节点不利指标(HDI) |  $S_{xy}=\frac{|\Gamma(x)\cap \Gamma(y)|}{max\{k_x,k_y\}}$   |
  |      ②Salton指标       | $S_{xy}=\frac{|\Gamma(x)\cap \Gamma(y)|}{\sqrt{k(x)\times k(y)}}$ |       ⑦LHN-I指标       |  $S_{xy}=\frac{|\Gamma(x)\cap \Gamma(y)|}{k(x)\times k(y)}$  |
  |      ③Jaccard指标      | $S_{xy}=\frac{|\Gamma(x)\cap \Gamma(y)|}{|\Gamma(x)\cup \Gamma(y)|}$ |   ⑧优先链接指标(PA)    |                   $S_{xy}=k(x)\times k(y)$                   |
  |     ④Sorensen指标      |    $S_{xy}=\frac{2|\Gamma(x)\cap \Gamma(y)|}{k(x)+k(y)}$     |  ⑨Adamic-Adar指标(AA)  | $S_{xy}=\sum_{z\in\Gamma(x)\cap\Gamma(y)}\frac{1}{log|\Gamma(z)|}$ |
  | ⑤大度节点有利指标(HPI) | $S_{xy}=\frac{|\Gamma(x)\cap \Gamma(y)|}{min\{k(x),k(y)\}}$  |   ⑩资源分配指标(RA)    |   $S_{xy}=\sum_{z\in\Gamma(x)\cap\Gamma(y)}\frac{1}{k(z)}$   |

  - $\Gamma(x)$表示顶点$x$的邻居集
  - 表中的$k(x)=|Γ(x)|=k_x$为节点$x$的度

##### CN指标

- 通用邻居(Common Neighbors, CN)相似度是最简单的最简单的近邻指标，它通过观察两个样本之间共有的邻居数量来决定它们是否相似，其表达式为：
  $$
  S_{xy}=|N(x)\cap N(y)|
  $$

  - 其中，$N(x)$表示$x$样本的邻居集
  - 在一个社交网络的好友推荐场景中，CN相似度也是用户$x$与用户$y$之间共同好友的数量；而在短视频推荐场景中，CN相似度可以认为是用户$x$与用户$y$都喜欢的短视频数量

- CN相似度就是x节点的一阶邻居集与y节点的一阶邻居集的交集数量
  - 两个节点一阶邻居的交集数量其实就等于他们之间的二阶路径数
  - 所以CN相似度公式可以写成一个新的形式：$S_{xy}=P_{xy}^{(2)}$
    - 式子右边的$P_{xy}^{(2)}$就代表节点x与节点y之间的二阶路径数

<img src="/Users/zhangyuxin/Library/Application Support/typora-user-images/截屏2023-08-13 16.17.55.png" alt="截屏2023-08-13 16.17.55" style="zoom:50%;" />

- 上图的链路预测二阶路径表

  | 二阶路径数 | 节点1 | 节点2 | 节点3 | 节点4 | 节点5 | 节点6 | 节点7 | 节点8 | 节点9 |
  | :--------: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
  |   节点1    |   0   |   0   |   0   |   2   |   0   |   1   |   1   |   0   |   0   |
  |   节点2    |   0   |   0   |   2   |   1   |   1   |   0   |   1   |   1   |   1   |
  |   节点3    |   0   |   2   |   0   |   0   |   1   |   0   |   1   |   1   |   1   |
  |   节点4    |   2   |   1   |   0   |   0   |   0   |   1   |   1   |   0   |   0   |
  |   节点5    |   0   |   1   |   1   |   0   |   0   |   0   |   0   |   0   |   1   |
  |   节点6    |   1   |   0   |   0   |   1   |   0   |   0   |   0   |   0   |   0   |
  |   节点7    |   1   |   1   |   1   |   1   |   0   |   0   |   0   |   1   |   1   |
  |   节点8    |   0   |   1   |   1   |   0   |   0   |   0   |   1   |   0   |   1   |
  |   节点9    |   0   |   1   |   1   |   0   |   1   |   0   |   1   |   1   |   0   |

  - 该表也是所有节点之间的CN相似度矩阵，且可被视作是另一个有权图的邻接矩阵，中间的数字正是边的权重
  - 通常被称为原图的二阶路径图
  - 它的邻接矩阵记作$A^{(2)}$
  - 所以原图中所有节点相似度矩阵S在目前计算环境下可写成：$S=A^{(2)}$

- CN相似度的代码非常简单，仅需计算两个集合的交集长度，代码如下：

  ```python
  # CN(Common Neighbors)相似度
  def CN(set1, set2):
      return len(set1 & set2)
  ```

  **注意**：CN相似度的值是[0，∞]，所以无法单从某两个样本的CN相似度指标的值来判断它们是否相似，而必须对样本间的两两CN相似度比较后才能判断。当然也可对所有的CN相似度做归一化处理

##### Jaccard指标

假设用户A喜欢电影1、2和3。用户B喜欢电影1～10。用户C喜欢电影2、3和4。如果仅考虑CN相似度指标，则用户A与用户B的CN相似度为3，用户A与用户C的相似度为2。如此一来用户B对于用户A来讲更相似

但是直觉告诉我们似乎这样不是很合理，因为用户B喜欢的电影太多了，与其他用户有重叠的部分自然会多，而用户A与C分别仅仅喜欢3部电影，并且在此基础上还有两部电影重叠。似乎用户C相对用户B来讲更应该是用户A的相似用户

- 所以Jaccard在20世纪初提出的Jaccard指标用于当前场景似乎更加准确。公式如下：
  $$
  S_{xy}=\frac{|N(x)\cap N(y)|}{|N(x)\cup N(y)|}
  $$
  由上式可以看出，Jaccard指标是在CN指标的基础上除以样本间的并集。这样就考虑了样本本身的集合越多，对于相似判断的权重就会越低

- Jaccard相似度的代码如下：

  ```python
  # Jaccard相似度
  def Jaccard(set1, set2):
      return len(set1 & set2) / len(set1 | set2)
  ```

  将上文提到的A、B和C，3种用户数据代入，则可以得到用户A与用户B的Jaccard相似度为0.3，用户A与用户C之间的相似度为0.5，所以用户C与用户A更加相似。

**注意**：Jaccard相似度的取值范围为[0，1]，越接近1则表示越相似。不需要再做归一化处理。

##### Salton指标

- Salton指标又称余弦相似度，即Cos相似度
  
- Cos相似度即两个向量在空间里的夹角余弦值
  
  - 夹角余弦值越接近于1代表夹角越接近0°，即在空间中的方向越相近
  - 反之，余弦值越接近于0代表夹角接近90°，即在空间中越接近正交
  - 如果接近于-1则代表完全反方向的向量
  - Cos相似度的取值范围为[-1，1]
  
- Cos相似度的公式如下：
  $$
  S_{xy}\frac{X·Y}{||X||~||Y||}
  $$
  即**两个向量的内积除以L2范数的乘积。**

- 向量间Cos相似度的代码如下：

  ```python 
  # 两个向量间的Cos相似度
  def cos4vector(v1, v2):
      return (np.dot(v1, v2)) / (np.linalg.norm(v1) * np.linalg.norm(v2))
  ```

- 两个集合间Cos相似度的公式如下：
  $$
  S_{xy}=\frac{|N(x)\cap N(y)|}{\sqrt{|N(x)|\times |N(y)|}}
  $$
  可以把集合视作1与0的向量

  - 例如：$N(x)=\{1,2,3,5\},~N(y)=\{1,4,5,6\}$，则$X$的向量可表示为$[1,1,1,0,1,0]$，$Y$的向量可表示为$[1,0,0,1,1,1,]$
    - $X$与$Y$向量的点乘是元素值同时为1的对应位置的和，也是两个集合的交集，而$X$向量的模长，也是向量中为1的数量开根号，而为1的数量是本身集合的长度

- 集合间Cos相似度的代码如下：

  ```python
  # 两个集合间的Cos相似度
  def cos4set(set1, set2):
      return len(set1 & set2) / (len(set1) * len(set2)) ** 0.5
  ```

###### 范数

- 范数是用来衡量一个向量大小的物理量，记作$L_p$或者$||X||_p$，定义如下：
  $$
  ||X||_p=(\sum_i|x_i|^p)^{\frac{1}{p}}
  $$
  

  其中，$p\in R,~p\geqslant1$

- 当$p=1$时，是L1范数：
  $$
  ||X||_1=\sum_i|x_i|
  $$
  即$X$向量中所有元素绝对值的和

- 当$p=2$时，是L2范数：
  $$
  ||X||_2=(\sum_i|x_i|^2)^{\frac{1}{2}}
  $$
  即所有元素平方和再求根

  - L2范数也被称为欧几里得范数(Euclidean Norm),因为它表示的是原点与该向量的欧几里得距离
  - 在机器学习中很常用，有时会省略下标2，直接用$||X||$表示，通常也被称为向量的模长
  - 有时也可通过向量与自身转置的点积来计算，记作$X^TX$

 ##### Pearson相似度

- 皮尔逊(Pearson)相似度又称皮尔逊相关系数，用于衡量两个变量之间的相关程度
  - 取值范围为[-1，1]
    - 1代表完全相关，0代表毫无关系，-1代表完全负相关

- Pearson相似度的基准公式为：
  $$
  S_{xy}=\frac{cov(X,Y)}{\sigma X\sigma Y}
  $$

  - 其中，$cov(X,Y)$代表协方差矩阵，公式如下：
    $$
    cov(X,Y)=\sum_{i=1}^n(X_i-\overline{X})(Y_i-\overline{Y})
    $$

  - $\sigma X$代表$X$的标准差：
    $$
    \sigma X=\sqrt{\sum^n_{i=1}(X_i-\overline X)^2}
    $$

    - $\overline X$是$X$的期望也是平均值，所以Pearson相似度可展开为：
      $$
      S_{xy}=\frac{\sum^n_{i=1}(X_i-\overline X)(Y_i-\overline Y)}{\sqrt{\sum^n_{i=1}(X_i-\overline X)^2}\sqrt{\sum^n_{i=1}(Y_i-\overline Y)^2}}
      $$

  - Pearson相似度的代码如下：

    ```python
    # 两个向量间的Pearson相似度
    def pearson(v1, v2):
        v1_mean = np.mean(v1)
        v2_mean = np.mean(v2)
        p = np.dot(v1 - v1_mean, v2 - v2_mean) / (np.sqrt(v1 - v1_mean) * np.sqrt(v2 - v2_mean))
        return p
    ```

##### Pearson相似度与Cos相似度之间的联系

- Cos相似度的公式展开为
  $$
  Cos_{xy}=\frac{X·Y}{||X||~||Y||}=\frac{\sum^n_{i=1}X_iY_i}{\sqrt{\sum^n_{i=1}X_i^2}\sqrt{\sum^n_{i=1}Y_i^2}}
  $$

  - 如果设向量$X'$和$Y'$并令$X'=X-\overline X,~Y'=Y-\overline Y$，Cos公式和Pearson公式就变得很像

  - 所以Pearson相似度是先让每个向量的值都减去该向量所有值的平均值，然后求夹角余弦值。这个操作其实等价于先对数据做标准化的处理，然后求取余弦相似度，代码如下：

    ```python
    # 两个向量间的Pearson相似度
    def pearsonSimple(v1, v2):
        v1 -= np.mean(v1)
        v2 -= np.mean(v2)
        return cos4vector(v1, v2)
    ```

- 对数据做标准化处理的意义在于：
  - 用实际的数据来说明，例如用户A和用户B对物品1、物品2及物品3的评分分别是A：[1，3，2]和B：[8，9，1]。如果直接求取它们的Cos相似度，则等于0.82。这是一个很大的数，但是单从数据上看用户A和用户B不该有那么高的相似度，因为显然它们对物品的评分差异是很大的，所以对数据进行标准化处理后，也是将用户对每个物品的评分减去该用户对所有物品的平均得分，再求夹角余弦值可得0.11，即Pearson相似度，显然这更加合理

- **注意**：不是所有情况下Pearson相似度都优于Cos相似度
  - 如果只是求两个集合间的相似度，则Pearson相似度无意义。因为Pearson相似度更能体现的是数据的线性相关性，假设每个用户对物品的评分具备时间属性，例如用户越早的评分记录在向量中靠前的位置，越晚的评分记录在向量中靠后的位置。此时的Pearson相似度就能完美地体现出不同用户对物品喜好演化过程的相似度

##### Sorensen指标

- Sorensen指标常用于生态学研究

- 其定义为：
  $$
  S_{xy}=\frac{2|\Gamma(x)\cap \Gamma(y)|}{k(x)+k(y)}
  $$

##### 大度节点有利指标(Hub promoted index, HPI)

- 大度节点有利指标(Hub promoted index, HPI)被用来定量刻画新陈代谢网络中每对反应物的拓扑相似程度

- 其定义为：
  $$
  S_{xy}=\frac{|\Gamma(x)\cap \Gamma(y)|}{min\{k(x),k(y)\}}
  $$

- 由于分母只由度较小的节点决定，在此定义下可知大度节点(hub)与其他节点之间更容易具有高的相似性

##### 大度节点不利指标(hub depressed index, HDI)

- 大度节点不利指标(hub depressed index, HDI)定义与HPI相似，只是分母取两端节点度的最大值

- 其定义为：
  $$
  S_{xy}=\frac{|\Gamma(x)\cap \Gamma(y)|}{max\{k(x),k(y)\}}
  $$

##### LHN-I指标

- LHN-I指标是由Leicht、Holme和Newman提出的

- 其定义为：
  $$
  S_{xy}=\frac{|\Gamma(x)\cap \Gamma(y)|}{k(x)\times k(y)}
  $$

  - 其中，分母$k(x)\times k(y)$正比于节点$v_x$和节点$v_y$共同邻居数的期望值，即$E(|\Gamma(x)\cap \Gamma(y)|)$

##### PA指标

- PA(Preferential Attachment)指标是基于BA无标度网络模型中新加入节点倾向于和度大的节点相连的优先连接机制而提出的

- 在这种机制的链路预测中，每一步首先去除一条链接，然后再添加一条链接，新链接连接节点和的概率就正比于两节点度的乘积

- 由此可以定义两节点间的优先连接相似性：
  $$
  S_{xy}=k(x)\times k(y)
  $$

##### AA指标

- AA(Adamic-Adar)指标的基本思想是度小的共同邻居节点的贡献大于度大的共同邻居节点，因此根据共同邻居节点的度为每个节点赋予一个权重值
  - 例如在微博中受关注较多的人往往是某个领域的专家或名人，因此共同关注他们的人之间可能并不拥有特别相似的兴趣
    - 一个中学生和一个企业家都有可能是某个明星的粉丝
  - 相反，如果两个人共同关注了一个粉丝很少的人（非名人），那么说明这两个人确实具有相同的兴趣爱好或者重叠的社交圈，因此有更高概率相连
  - 又如在推荐系统中，共同购买冷门产品的两个用户往往比共同购买热门产品的用户更相似

- AA指标根据共同邻居节点的度为每个节点赋予一个权重值，该权重等于该节点的度的对数分之一，即AA指标的定义为：
  $$
  \sum_{z\in\Gamma(x)\cap\Gamma(y)}\frac{1}{log|\Gamma(z)|}=\sum_{z\in\Gamma(x)\cap\Gamma(y)}\frac{1}{log~k(z)}
  $$

##### RA指标

- RA(Resource allocation)指标是受网络资源分配过程的启发，周涛等人提出的“**资源分配指标 Resource allocation**”

- 其与AA指标有异曲同工之妙

  - RA和AA指标最大的区别就在于赋予共同邻居节点权重的方式不同，前者是以 1/k 的形式递减，后者是以 1/logk 的形式递减
  - 可见，当网络的平均度较小时，RA和AA差别不大；但是当平均度较大时，就有很大的区别了
  - 研究显示，RA指标在刻画加权网络和社区挖掘应用中的表现胜过AA指标，图灵奖得主Hopdroft的一篇论文也显示，可以利用社区等中观结构信息改进局部指标，其中RA指标改进后的效果最佳

- 其定义为：
  $$
  S_{xy}=\sum_{z\in\Gamma(x)\cap\Gamma(y)}\frac{1}{|\Gamma(z)|}=\sum_{z\in\Gamma(x)\cap\Gamma(y)}\frac{1}{k(z)}
  $$

#### 基于路径的相似性指标

基于共同邻居的相似性指标的优势在于计算复杂度较低，但是由于使用的信息非常有限，预测精度受到限制，因此发展出了基于路径的相似性指标，主要有3个，分别是局部路径指标 Local path、Katz指标和LHN-II指标

##### 局部路径指标

- 周涛等人在共同邻居的基础上考虑三阶路径的因素，提出了基于局部路径的相似性指标，其定义为：
  $$
  S=A^2+\alpha A^3
  $$

  - 其中$\alpha$为可调参数，A表示网络的邻接矩阵，$(A^3)_{xy}$表示节点$V_x$和$V_y$之间长度为3的路径数目
  - 当$\alpha=0$时，LP指标就退化为CN指标
    - CN 指标本质上也可看成基于路径的指标，只是它仅考虑了二阶路径数目

- 局部路径指标可以扩展为更高阶的情形，即考虑n阶路径的情况：
  $$
  S^n=A^2+\alpha A^3+\alpha ^2A^4+...+\alpha ^{n-2}A^n
  $$

  - 随着n的增加，局部路径指标的计算复杂度越来越大
  - 一般而言，考虑n阶路径的计算复杂度为$O(N<k>^n)$
  - 但是当 n 趋于无穷大的时候，局部路径指标相当于考虑网络全部路径的Katz指标，此时计算量反而有可能下降，因为可转变为计算矩阵的逆

##### Katz指标

- Katz指标考虑了所有网络的路径，因此其定义为：
  $$
  S_{xy}=\sum_{l=1}^\infty\alpha^l|paths_{(x,y)}^{<l>}|=\alpha A_{xy}+\alpha^2 (A^2)_{xy}+\alpha^3 (A^3)_{xy}+...
  $$

  - 其中$\alpha>0$为控制路径权重的可调参数，$|paths_{(x,y)}^{<l>}|$表示连接节点$V_x$和 $V_y$的路径中长度为$l$的路径数

  - 要使上述级数收敛，则参数$\alpha$应小于邻接矩阵最大特征值的倒数

  - 此定义还可以表示为：
    $$
    S=(I-\alpha·A)^{-1}-I
    $$

    - 当参数$\alpha$很小时，高阶路径的贡献也较小，使得 Katz 指标的预测结果接近于局部路径指标

##### LHN-II指标

- LHN-II 指标是 Leicht、Holme和Newman提出的另一种相似性计算方法

- 他的基本思想是基于**一般等价Regular equivalence** 提出的

  - 与结构等价不同，一般等价的定义更广泛

  - 在一般等价的定义下，如果两个节点所连接的节点之间相似，那么这两个节点也相似，即使它们之间没有共同的邻居节点

  - 下图给出了一个简单的例子来帮助我们理解结构等价和一般等价的区别

    <img src="/Users/zhangyuxin/Library/Application Support/typora-user-images/一般等价和结构等价示例.png" alt="一般等价和结构等价示例" style="zoom:67%;" />

    - 根据结构等价的定义，在公司A内部，由于员工a和员工b都认识员工e和员工f(即它们有两个共同邻居节点)，于是称员工a和员工b是结构等价的.同理，员工e和员工f也是结构等价的，同样地，在公司B中，员工c和员工d是结构等价的，员工g和员工h也是结构等价的
    - 现在考虑员工a和员工c是否相似？根据一般等价的定义，由于员工a连接的节点e、f与员工c连接的节点g、h分别是相似的，那么我们可以认为员工a和员工c也是相似的.同理，在一般等价的意义下，员工b和员工d也是相似
    - 从这个例子可以看出，结构相似性或称结构等价更强调的是两个节点是否在同一个环境下，也就是说是否连接了相同的节点；然而一般等价考虑的是这两个节点是否处于同样的角色，即使他们没有相同的邻居节点，但是由于各自的邻居节点之间本身相似，这两个节点也相似

  - 一般等价意义下的相似性是LHN-II指标的核心

  - 从一般等价的角度考虑，LHN-II指标认为如果两个节点的邻居节点之间是相似的，那么这两个节点也是相似的

- 它与Katz指标的区别主要是把Katz指标中的$(A^n)_{xy}$变成$(A^n)_{xy}$的期望值，注意到$(A^n)
  _{xy}$的期望值为$E[(A^l)_{xy}]=\frac{k_xk_y}{M}\lambda^{l-1}_1$

  - 其中$\lambda_1$为矩阵A的最大特征值，则LHN-II指标的表达式如下：
    $$
    S_{xy}=\delta_{xy}+\sum_{l=1}^\infty\Phi^l\frac{(A^l)_{xy}}{E[(A^l)_{xy}}=\delta_{xy}+\frac{2M}{k_xk_y}\sum_{l=1}^\infty\Phi^l\lambda^{l-1}_1(A^l)_{xy}=[1-\frac{2M\lambda_1}{k_xk_y}]\delta_{xy}+\frac{2M\lambda_1}{k_xk_y}[(I-\frac{\Phi}{\lambda_1}A)^{-1}]_{xy}
    $$

    - 其中$\delta_{xy}$为Kronecker $\delta$函数，$\Phi$为取值小于1的参数

    - 上式最后一个等式的第一项是可以去掉的对角阵，从而相似性矩阵可以写为：
      $$
      S=2M\lambda_1D^{-1}(I-\frac{\Phi}{\lambda_1}A)^{-1}D^{-1}
      $$

      - 其中*D*为度值矩阵，$D_{xy}=\delta_{xy}·k_x$

#### 基于随机游走的节点相似性指标

还有相当数量的相似性指标是基于随机游走过程定义的，包括平均通勤时间Average commute time、Cos+指标、有重启的随机游走Random walk with restart、SimRank指标、局部随机游走指标Local random walk和有叠加效应的随机游走指标Superposed random walk等

##### 平均通勤时间 ACT

- 设$m(x,y)$为一个随机粒子从节点x到节点y平均需要走的步数，那么节点x和y的平均通勤时间定义为：
  $$
  n(x,y)=m(x,y)+m(y,x)
  $$

  - 其数值解可通过求该网络拉普拉斯矩阵$L$的伪逆$L^+$获得，即：
    $$
    n(x,y)=M(l^+_{xx}+l^+_{yy}-2l^+_{xy})
    $$

    - 其中$l^+_{xy}$表示矩阵$L^+$中相应位置的元素

    - 如果两个节点的平均通勤时间越小，那么两个节点越进阶

    - 由此，定义基于ACT的相似性为(在此可忽略常数M)：
      $$
      S_{xy}=\frac{1}{l^+_{xx}+l^+_{yy}-2l^+_{xy}}
      $$

##### Cos+指标

- Cos+指标，即余弦相似性指标

- 在由向量$v_x=\Lambda^{\frac{1}{2}}U^Te_x$展开的欧式空间内，$L^+$中的元素$l^+_{xy}$可表示为两向量$v_x$和$v_y$的内积，即$l^+_{xy}=v_x^Tv_y$

  - 其中$U$是一个标准正交矩阵，由$L^+$特征向量按照对应的特征根从大到小排列，$\Lambda$为以特征根为对角元素的对角矩阵，$e_x$表示一个一维向量且只有第x个元素为1，其他都为0

  - 由此定义余弦相似性如下：
    $$
    S_{xy}=cos(x,y)^+=\frac{l_{xy}^+}{\sqrt{l^+_{xx} \cdot l^+_{yy}}}
    $$

##### RWR指标

- RWR即重启的随机游走指标

- 这个指标可以看成是PageRank算法的拓展应用

  - 它假设随机游走粒子在每走一步的时候都以一定概率返回初始位置

    - 设粒子返回概率为$1-c$，$P$为网络的马尔科夫概率转移矩阵，其元素$P_{xy}=\frac{a_{xy}}{k_x}$表示节点x处的粒子下一步走到节点y的概率

      - 其中如果节点x和节点y相连，则$a_{xy}=1$，否则$a_{xy}=0$

    - 某一粒子初始时刻在节点x处，那么t+1时刻该粒子到达网络各个节点的概率向量为：
      $$
      q_x(t+1)=c\cdot P^Tq_x(t)+(1-c)e_x
      $$

      - 其中$e_x$表示初始状态(其定义与Cos+中相同)，上式的稳态解为：
        $$
        q_x=(1-c)(I-cP^T)^{-1}e_x
        $$

        - 其中元素$q_{xy}$为从节点x出发的粒子最终有多少概率走到节点y

        - 由此定义RWR相似性如下：
          $$
          S_{xy}=q_{xy}+q_{yx}
          $$

##### SimRank指标

- 它的基本假设是:如果两节点所连接的节点相似，那么这两个节点就相似

- 其自洽定义式为：
  $$
  S_{xy}=C\frac{\sum_{v_z\in\Gamma(x)v_z'\in\Gamma(y)}s_{zz'}^{SimR}}{k_xk_y}
  $$

  - 其中假定$s_{xx}=1,~C\in[0,1]$为相似性传递时的衰减参数

- SimR指标同时考虑了结构等价和一般等价
  - 当节点x的邻居z同时也是节点y的邻居时，该指标考虑了结构等价
  - 当节点x不等于节点y时，该指标概率了一般等价
  - 可以证明，SimR指标可以用来描述两个分别从节点x和y出发的粒子平均过多久会相遇

##### LRW指标

- LRW即局部随机游走指标

- 上述4种随机游走指标是基于全局的随机游走，这类指标往往计算复杂度很高，因此很难在大规模网络上应用。刘伟平和吕琳媛提出了一种基于网络局部随机游走的相似性指标LRW，它只考虑有限步数的随机游走过程

  - 一个粒子t时刻从节点x出发，定义$\pi_{xy}(t)$为t+1时刻这个粒子正好走到节点y的概率，那么可得到系统演化方程：
    $$
    \pi_x(t+1)=P^T\pi_x(t),~t=0,1,...
    $$

    - 其中$\pi_x(0)$为一个$N_{x1}$的向量，只有第x个元素为1，其余为0，即$\pi_x(0)=e_x$

    - 设定各个节点的初始资源分布为$q_x$，那么基于t步随机游走的相似性为：
      $$
      S_{xy}=q_{xy}+q{yx}
      $$

##### SRW指标

- SRW即叠加的局部随机游走指标

- 这个指标给邻近目标节点的点更多的机会与目标节点相连，充分考虑了很多真实网络连接上的局域性特点

- 它是在LRW的基础上，将t步及其以前的结果加总，便得到SRW的值，即：
  $$
  S_{xy}^{SRW}(t)=\sum_t^{l=1}S_{xy}^{LRW}(l)=q_x\sum_t^{l=1}\pi_{xy}(l)+q_y\sum_t^{l=1}\pi_{yx}(l)
  $$

### 基于似然分析的链路预测

- 除了最简单的分析框架“基于相似性的链路预测方法”之外，还有另一种最复杂的链路预测分析框架：基于似然分析的链路预测
- 这个框架本身远远复杂于基于相似性的框架，而且框架中的每一个组成成分自身都非常复杂，但这个框架下衍生出的算法除了给出链路预测的结果之外，还给出了我们对于网络结构的深刻洞见
  - 例如“**层次结构模型 Hierachical structure model**”给出了网路层次组织形态的定量刻画
  - “**随机分块模型 Stochastic block model**”类似于社区分解一样把网络划分成了若干子块
  - 这些对理解网络结构特征方面的贡献，是基于相似性的框架所不能提供的

- 基于似然分析的链路预测的基本思路是：根据网络结构的产生和组织方式以及目前已经观察到的链路计算网络的似然值，并认为真实的网络使得网络似然值最大，然后再根据网络似然最大化计算每一对未连接的节点产生连边的可能性

#### 层次结构模型 HSM

- 层次结构模型假设真实的网络都存在某种层次性，网络的连接则可看作是这种内在层次结构的反映

- 该方法的计算步骤如下：

  1. 一个N个节点的网络可以用一个包含N个叶子节点的族谱树表示，这N个叶子节点将由N-1个非叶子节点连接起来，其中每个非叶子节点都有一个概率值，则两个叶子节点连接的概率就等于他们最近共同祖先节点的概率值。给定一个族谱树，将网络的似然值最大化，就可以得到非叶子节点的概率值，并由此计算出这一个族谱树所对应的网络最大的似然值。

  2. 使用马尔科夫链蒙特卡洛方法对网络不同的族谱树进行抽样，使得每个族谱树出现的频次正比于该树对应的最大的网络似然值。两个节点连边的概率就等于所有抽样出来的族谱树中两个节点连边概率的平均值。注意似然高的族谱树会出现多次，因此对这个平均概率的贡献也大。

  3. 把平均连边概率看做是相似性框架中的相似性指标$S_{xy}$，按照这个值从大到小排序，排在前面的就是预测的连边。

  - 需要指出的是，层次结构模型在处理具有明显层次结构的网络（如恐怖袭击网络和草原食物链）时具有较高的精确度，但在处理一般性网络时效果并不一定突出

#### 随机分块模型 SBM

- 与层次结构模型的思想类似，随机分块模型假设网络中的节点可以被分为若干集合，两个节点间连接的概率只与相应的集合有关
  - 换句话说，同一个群众所有节点的地位是相同的
- 随机分块模型特别适合刻画节点所属群的成员身份对于其连接行为有关键影响的情况
  - 例如，著名的“角色模型”就包含了随机分块模型的理念

- 随机分块模型的效果要好于层次结构模型
  - 同时，该方法还可以剔除网络的错误连边，如纠正蛋白质相互作用网络中的错误连边

#### 闭路模型 Loop model

- 似然分析的更一般的框架是：给定一个网络系统，特定网络哈密顿量的负指数被统计分配函数归一化后，就得到这个网络出现的似然，一条未被观察到的连边存在的可能性就等于添加这条连边后网络的似然值
- 闭路模型考虑网络结构形成中的“局部性原则”，并由此定义了网络的哈密顿量
- 实验表明，闭路模型的预测精度大于层次结构模型和随机分块模型

### 基于机器学习的链路预测

用机器学习的思路进行链路预测，主要分为基于特征分类方法、基于概率图模型方法和基于矩阵分解方法三大类

#### 基于特征分类方法

- 网络中的链路预测问题可以看成机器学习中的分类问题，其中每个数据点对应一对节点之间关系的标记，假定两个节点之间存在连边，则数据点的值为+1，否则为-1
- 特征选取是分类问题中最重要的问题之一，目前研究较多的主要包括基于节点与边的属性特征和基于节点所处网络的拓扑结构特征
  - 如CN、AA、Katz、最短路径等
- 哈桑 Hasan 等人提取合著网络中科学家研究领域的关键词作为特征，用监督学习中一些常用的**分类算法**对缺失的连边进行较为精准的预测，其中以支持向量机方法表现最佳
  - 如决策树、K近邻法、多层感知器、支持向量机、径向基网络

#### 基于概率图模型

- 基于概率图模型的链路预测方法使用图模型来表达节点之间的连边概率，根据模型中概率依赖关系，可分为有向无环的贝叶斯网络和无向的马尔科夫网络

- 随机分块模型和层次结构模型分别是典型的基于贝叶斯网络和基于马尔科夫网络的链路预测方法
- 基于概率图模型的链路预测方法有很多，典型的还有有监督随机游走链路预测算法
  - 将网络结构信息与节点和连边的属性信息结合起来，给每条边分配不同的转移概率
  - 与其他有监督的机器学习方法相比，该算法无须知道网络的特征和相关的领域知识

#### 基于矩阵分解

- 基于矩阵分解的链路预测方法是基于网络邻接矩阵的代数谱变换而得到的方法，来预测网络中链路的存在性和链路的权值
- 相比其他几种机器学习方法，该方法要学习的参数少很多，但计算复杂度较高
- 链路预测问题可以视为邻接矩阵填充问题，并可以通过双线性回归模型将节点和链路的显特征和隐特征结合起来用矩阵分解方法解决。边缘去噪模型将根据给定关系矩阵求未知边或丢失边的问题变成一个矩阵降噪去噪问题
- 矩阵分解算法的基本思想是认为用户对物品的偏好是外在表现，内在是用户对主题的偏好
  - 而主题对不同物品又有不同的权重，通过用户->主题->物品这条链路才形成用户对物品的偏好

- 矩阵分解的公式：U=PQ
  - 其中U表示用户对不同物品的偏好矩阵， P表示用户对不同主题的偏好矩阵， Q表示不同主题对应用的权重
